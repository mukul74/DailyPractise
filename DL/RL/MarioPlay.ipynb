{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Mario\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating The Environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done = True\n",
    "# for step in range(100000):\n",
    "#     if done:\n",
    "#         env.reset()\n",
    "#     state, reward, done, info  = env.step(env.action_space.sample())\n",
    "#     env.render()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarw\\anaconda3\\envs\\myspace\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# PreProcess The Environment\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 240, 256, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info  = env.step([env.action_space.sample()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'C:/D_drive/Practise/DL/RL/Mario/TRAIN/'\n",
    "LOG_DIR = 'C:/D_drive/Practise/DL/RL/Mario/LOGS/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=100000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.000001, n_steps=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('C:/D_drive/Practise/DL/RL/Mario/TRAIN/best_model_300000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarw\\anaconda3\\envs\\myspace\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Loop through the game\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m: \n\u001b[1;32m----> 6\u001b[0m     action, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(state)\n\u001b[0;32m      7\u001b[0m     state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m      8\u001b[0m     env\u001b[39m.\u001b[39mrender()\n",
      "File \u001b[1;32mc:\\Users\\agarw\\anaconda3\\envs\\myspace\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:589\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m    570\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    571\u001b[0m     observation: np\u001b[39m.\u001b[39mndarray,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    574\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    575\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[0;32m    576\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 589\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[1;32mc:\\Users\\agarw\\anaconda3\\envs\\myspace\\lib\\site-packages\\stable_baselines3\\common\\policies.py:336\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    333\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    335\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 336\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(observation, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[0;32m    337\u001b[0m \u001b[39m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[0;32m    338\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\agarw\\anaconda3\\envs\\myspace\\lib\\site-packages\\stable_baselines3\\common\\policies.py:629\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[1;34m(self, observation, deterministic)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, observation: th\u001b[39m.\u001b[39mTensor, deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    622\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[39m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_distribution(observation)\u001b[39m.\u001b[39mget_actions(deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32mc:\\Users\\agarw\\anaconda3\\envs\\myspace\\lib\\site-packages\\stable_baselines3\\common\\policies.py:656\u001b[0m, in \u001b[0;36mActorCriticPolicy.get_distribution\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_distribution\u001b[39m(\u001b[39mself\u001b[39m, obs: th\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Distribution:\n\u001b[0;32m    650\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[39m    Get the current policy distribution given the observations.\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \n\u001b[0;32m    653\u001b[0m \u001b[39m    :param obs:\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \u001b[39m    :return: the action distribution.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 656\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_features(obs)\n\u001b[0;32m    657\u001b[0m     latent_pi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_extractor\u001b[39m.\u001b[39mforward_actor(features)\n\u001b[0;32m    658\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n",
      "File \u001b[1;32mc:\\Users\\agarw\\anaconda3\\envs\\myspace\\lib\\site-packages\\stable_baselines3\\common\\policies.py:126\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \n\u001b[0;32m    122\u001b[0m \u001b[39m:param obs:\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_extractor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mNo features extractor was set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 126\u001b[0m preprocessed_obs \u001b[39m=\u001b[39m preprocess_obs(obs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation_space, normalize_images\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize_images)\n\u001b[0;32m    127\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_extractor(preprocessed_obs)\n",
      "File \u001b[1;32mc:\\Users\\agarw\\anaconda3\\envs\\myspace\\lib\\site-packages\\stable_baselines3\\common\\preprocessing.py:102\u001b[0m, in \u001b[0;36mpreprocess_obs\u001b[1;34m(obs, observation_space, normalize_images)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39mPreprocess observation to be to a neural network.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39mFor images, it normalizes the values by dividing them by 255 (to have values in [0, 1])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(observation_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m--> 102\u001b[0m     \u001b[39mif\u001b[39;00m is_image_space(observation_space) \u001b[39mand\u001b[39;00m normalize_images:\n\u001b[0;32m    103\u001b[0m         \u001b[39mreturn\u001b[39;00m obs\u001b[39m.\u001b[39mfloat() \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m\n\u001b[0;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m obs\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Users\\agarw\\anaconda3\\envs\\myspace\\lib\\site-packages\\stable_baselines3\\common\\preprocessing.py:49\u001b[0m, in \u001b[0;36mis_image_space\u001b[1;34m(observation_space, check_channels)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39m# Check the value range\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39many(observation_space\u001b[39m.\u001b[39mlow \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m np\u001b[39m.\u001b[39many(observation_space\u001b[39m.\u001b[39;49mhigh \u001b[39m!=\u001b[39;49m \u001b[39m255\u001b[39;49m):\n\u001b[0;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39m# Skip channels check\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start the game \n",
    "state = env.reset()\n",
    "# Loop through the game\n",
    "while True: \n",
    "    \n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('myspace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf10472f9094a5f15328377ff1e1069a069e10002bff592953a789dfa676fdea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
